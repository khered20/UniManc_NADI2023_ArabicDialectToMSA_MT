{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPyDpc0FNU/XCZZG76/YzLz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khered20/UniManc_NADI2023_ArabicDialectToMSA_MT/blob/main/AraT5v2_DialecticalArabicToMSA_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg8Sjx2GVCwH",
        "outputId": "8fbada58-beed-483f-ec30-85164e814ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=2c6881d2aed363ec43e5fd38d057061384e235634c96f1ef61ef6f4012dcdaf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: transformers==4.24.0 in /usr/local/lib/python3.10/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (0.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate datasets sacrebleu sentencepiece evaluate rouge_score\n",
        "!pip install --upgrade transformers==4.24.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fnY-4uKTAcL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/run_trainier_seq2seq_huggingface.py?token=AA4R7KKKCBPS5WX4BJSWNFTBG2OPK -O run_trainier_seq2seq_huggingface.py\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/eval_squad.py?token=AA4R7KOBBXSPCDXRRHJ3RW3BG2RF4 -O eval_squad.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tumlp46VKHQ",
        "outputId": "92f9e817-f98f-4c9e-85c2-51757820d177"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-18 15:20:12--  https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/run_trainier_seq2seq_huggingface.py?token=AA4R7KKKCBPS5WX4BJSWNFTBG2OPK\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36637 (36K) [text/plain]\n",
            "Saving to: ‘run_trainier_seq2seq_huggingface.py’\n",
            "\n",
            "run_trainier_seq2se 100%[===================>]  35.78K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2023-10-18 15:20:13 (6.37 MB/s) - ‘run_trainier_seq2seq_huggingface.py’ saved [36637/36637]\n",
            "\n",
            "--2023-10-18 15:20:13--  https://raw.githubusercontent.com/UBC-NLP/araT5/main/examples/eval_squad.py?token=AA4R7KOBBXSPCDXRRHJ3RW3BG2RF4\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7689 (7.5K) [text/plain]\n",
            "Saving to: ‘eval_squad.py’\n",
            "\n",
            "eval_squad.py       100%[===================>]   7.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-18 15:20:13 (92.6 MB/s) - ‘eval_squad.py’ saved [7689/7689]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a joint model for all regional dialects (J-R).\n",
        "<br> prepare a csv file for the NADI dev dataset where column \"0\" is the dialectical text and \"1\" is the corresponding MSA"
      ],
      "metadata": {
        "id": "6RjneLduRrcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_trainier_seq2seq_huggingface.py \\\n",
        "        --learning_rate 5e-5 \\\n",
        "        --max_target_length 128 --max_source_length 128 \\\n",
        "        --per_device_train_batch_size 16 --per_device_eval_batch_size 16 \\\n",
        "        --model_name_or_path \"UBC-NLP/AraT5v2-base-1024\" \\\n",
        "        --output_dir \"s2/AraT5_baseV2_100/regMADAR\" --overwrite_output_dir \\\n",
        "        --save_steps 1000 \\\n",
        "        --eval_steps 1000 \\\n",
        "        --num_train_epochs 2 \\\n",
        "        --train_file \"datasets/regMADARmsa.csv\" \\\n",
        "        --validation_file \"datasets/NADI_dev.csv\" \\\n",
        "        --test_file \"datasets/NADI_dev.csv\" \\\n",
        "        --task \"machine_translation\" --text_column \"0\" --summary_column \"1\" \\\n",
        "        --load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy steps --logging_strategy epoch --predict_with_generate\\\n",
        "        --do_train --do_eval --do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRgMaTh1WPPc",
        "outputId": "2033246f-a188-4254-89f8-7bc947cc17df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-18 15:45:04.676520: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-18 15:45:06.516811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "last_checkpoint None\n",
            "10/18/2023 15:45:11 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/18/2023 15:45:11 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=s2/AraT5_baseV2_100/regMADAR/runs/Oct18_15-45-11_b6e1481f22fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=epoch,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_bleu,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=s2/AraT5_baseV2_100/regMADAR,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=s2/AraT5_baseV2_100/regMADAR,\n",
            "save_on_each_node=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 19941.22it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1534.88it/s]\n",
            "Generating train split: 56505 examples [00:00, 270988.41 examples/s]\n",
            "Generating validation split: 400 examples [00:00, 74951.82 examples/s]\n",
            "Generating test split: 400 examples [00:00, 83986.86 examples/s]\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 88625095-79c3-4bc8-865c-c7ed5920500c)')' thrown while requesting HEAD https://huggingface.co/UBC-NLP/AraT5v2-base-1024/resolve/main/config.json\n",
            "10/18/2023 15:45:21 - WARNING - huggingface_hub.utils._http -   '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 88625095-79c3-4bc8-865c-c7ed5920500c)')' thrown while requesting HEAD https://huggingface.co/UBC-NLP/AraT5v2-base-1024/resolve/main/config.json\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5v2-base-1024\",\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110208\n",
            "}\n",
            "\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5d8d9863-dd42-4225-b2b6-a077a3fde08c)')' thrown while requesting HEAD https://huggingface.co/UBC-NLP/AraT5v2-base-1024/resolve/main/tokenizer_config.json\n",
            "10/18/2023 15:45:32 - WARNING - huggingface_hub.utils._http -   '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5d8d9863-dd42-4225-b2b6-a077a3fde08c)')' thrown while requesting HEAD https://huggingface.co/UBC-NLP/AraT5v2-base-1024/resolve/main/tokenizer_config.json\n",
            "loading file spiece.model from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/spiece.model\n",
            "loading file tokenizer.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/tokenizer_config.json\n",
            "loading weights file pytorch_model.bin from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5v2-base-1024.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Map:   0% 0/56505 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 56505/56505 [00:06<00:00, 8980.60 examples/s] \n",
            "Map: 100% 400/400 [00:00<00:00, 8252.77 examples/s]\n",
            "Map: 100% 400/400 [00:00<00:00, 9768.17 examples/s]\n",
            "[INFO] evlaute using  bleu score task name: machine_translation\n",
            "[INFO] early_stopping_num= 20\n",
            "***** checkpoint= None\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 56505\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7064\n",
            "  Number of trainable parameters = 367508736\n",
            "  0% 0/7064 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 14% 1000/7064 [05:30<31:59,  3.16it/s]***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:06,  3.73it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:07,  2.81it/s]\u001b[A\n",
            " 16% 4/25 [00:01<00:09,  2.17it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:10,  1.93it/s]\u001b[A\n",
            " 24% 6/25 [00:03<00:11,  1.70it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:11,  1.56it/s]\u001b[A\n",
            " 32% 8/25 [00:04<00:11,  1.53it/s]\u001b[A\n",
            " 36% 9/25 [00:04<00:09,  1.62it/s]\u001b[A\n",
            " 40% 10/25 [00:05<00:08,  1.77it/s]\u001b[A\n",
            " 44% 11/25 [00:05<00:07,  1.91it/s]\u001b[A\n",
            " 48% 12/25 [00:06<00:06,  2.00it/s]\u001b[A\n",
            " 52% 13/25 [00:06<00:05,  2.10it/s]\u001b[A\n",
            " 56% 14/25 [00:07<00:05,  2.12it/s]\u001b[A\n",
            " 60% 15/25 [00:07<00:04,  2.15it/s]\u001b[A\n",
            " 64% 16/25 [00:08<00:04,  2.14it/s]\u001b[A\n",
            " 68% 17/25 [00:08<00:03,  2.15it/s]\u001b[A\n",
            " 72% 18/25 [00:08<00:03,  2.20it/s]\u001b[A\n",
            " 76% 19/25 [00:09<00:02,  2.19it/s]\u001b[A\n",
            " 80% 20/25 [00:09<00:02,  2.12it/s]\u001b[A\n",
            " 84% 21/25 [00:10<00:02,  1.95it/s]\u001b[A\n",
            " 88% 22/25 [00:11<00:01,  1.99it/s]\u001b[A\n",
            " 92% 23/25 [00:11<00:00,  2.03it/s]\u001b[A\n",
            " 96% 24/25 [00:12<00:00,  2.01it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.114539861679077, 'eval_bleu': 13.7904, 'eval_gen_len': 12.68, 'eval_runtime': 13.1458, 'eval_samples_per_second': 30.428, 'eval_steps_per_second': 1.902, 'epoch': 0.28}\n",
            " 14% 1000/7064 [05:43<31:59,  3.16it/s]\n",
            "100% 25/25 [00:12<00:00,  2.03it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/regMADAR/checkpoint-1000\n",
            "Configuration saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-1000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/regMADAR/checkpoint-1000/spiece.model\n",
            " 28% 2000/7064 [11:43<26:15,  3.21it/s]***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:06,  3.56it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:08,  2.75it/s]\u001b[A\n",
            " 16% 4/25 [00:01<00:08,  2.40it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.20it/s]\u001b[A\n",
            " 24% 6/25 [00:02<00:09,  1.96it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:09,  1.87it/s]\u001b[A\n",
            " 32% 8/25 [00:03<00:08,  1.92it/s]\u001b[A\n",
            " 36% 9/25 [00:04<00:07,  2.01it/s]\u001b[A\n",
            " 40% 10/25 [00:04<00:07,  2.06it/s]\u001b[A\n",
            " 44% 11/25 [00:05<00:06,  2.11it/s]\u001b[A\n",
            " 48% 12/25 [00:05<00:06,  2.12it/s]\u001b[A\n",
            " 52% 13/25 [00:06<00:05,  2.19it/s]\u001b[A\n",
            " 56% 14/25 [00:06<00:05,  2.17it/s]\u001b[A\n",
            " 60% 15/25 [00:06<00:04,  2.19it/s]\u001b[A\n",
            " 64% 16/25 [00:07<00:04,  2.20it/s]\u001b[A\n",
            " 68% 17/25 [00:07<00:03,  2.20it/s]\u001b[A\n",
            " 72% 18/25 [00:08<00:03,  2.25it/s]\u001b[A\n",
            " 76% 19/25 [00:08<00:02,  2.19it/s]\u001b[A\n",
            " 80% 20/25 [00:09<00:02,  1.96it/s]\u001b[A\n",
            " 84% 21/25 [00:10<00:02,  1.72it/s]\u001b[A\n",
            " 88% 22/25 [00:10<00:01,  1.72it/s]\u001b[A\n",
            " 92% 23/25 [00:11<00:01,  1.65it/s]\u001b[A\n",
            " 96% 24/25 [00:12<00:00,  1.58it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.0718603134155273, 'eval_bleu': 12.8668, 'eval_gen_len': 12.925, 'eval_runtime': 13.1679, 'eval_samples_per_second': 30.377, 'eval_steps_per_second': 1.899, 'epoch': 0.57}\n",
            " 28% 2000/7064 [11:57<26:15,  3.21it/s]\n",
            "100% 25/25 [00:12<00:00,  1.68it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/regMADAR/checkpoint-2000\n",
            "Configuration saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-2000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-2000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/regMADAR/checkpoint-2000/spiece.model\n",
            " 42% 3000/7064 [17:59<24:35,  2.75it/s]***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:06,  3.67it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:08,  2.66it/s]\u001b[A\n",
            " 16% 4/25 [00:01<00:09,  2.17it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:10,  1.89it/s]\u001b[A\n",
            " 24% 6/25 [00:03<00:11,  1.69it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:11,  1.53it/s]\u001b[A\n",
            " 32% 8/25 [00:04<00:11,  1.51it/s]\u001b[A\n",
            " 36% 9/25 [00:04<00:09,  1.69it/s]\u001b[A\n",
            " 40% 10/25 [00:05<00:08,  1.82it/s]\u001b[A\n",
            " 44% 11/25 [00:05<00:07,  1.95it/s]\u001b[A\n",
            " 48% 12/25 [00:06<00:06,  2.01it/s]\u001b[A\n",
            " 52% 13/25 [00:06<00:05,  2.11it/s]\u001b[A\n",
            " 56% 14/25 [00:07<00:05,  2.10it/s]\u001b[A\n",
            " 60% 15/25 [00:07<00:04,  2.14it/s]\u001b[A\n",
            " 64% 16/25 [00:08<00:04,  2.16it/s]\u001b[A\n",
            " 68% 17/25 [00:08<00:03,  2.17it/s]\u001b[A\n",
            " 72% 18/25 [00:08<00:03,  2.23it/s]\u001b[A\n",
            " 76% 19/25 [00:09<00:02,  2.18it/s]\u001b[A\n",
            " 80% 20/25 [00:09<00:02,  2.11it/s]\u001b[A\n",
            " 84% 21/25 [00:10<00:02,  1.80it/s]\u001b[A\n",
            " 88% 22/25 [00:11<00:01,  1.85it/s]\u001b[A\n",
            " 92% 23/25 [00:11<00:01,  1.91it/s]\u001b[A\n",
            " 96% 24/25 [00:12<00:00,  1.91it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.085658073425293, 'eval_bleu': 13.3231, 'eval_gen_len': 12.7825, 'eval_runtime': 13.3329, 'eval_samples_per_second': 30.001, 'eval_steps_per_second': 1.875, 'epoch': 0.85}\n",
            " 42% 3000/7064 [18:13<24:35,  2.75it/s]\n",
            "100% 25/25 [00:12<00:00,  1.94it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/regMADAR/checkpoint-3000\n",
            "Configuration saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-3000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-3000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/regMADAR/checkpoint-3000/spiece.model\n",
            "{'loss': 2.17, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
            " 57% 4000/7064 [24:17<15:13,  3.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:06,  3.52it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:08,  2.72it/s]\u001b[A\n",
            " 16% 4/25 [00:01<00:08,  2.37it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.18it/s]\u001b[A\n",
            " 24% 6/25 [00:02<00:09,  1.97it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:09,  1.85it/s]\u001b[A\n",
            " 32% 8/25 [00:03<00:08,  1.90it/s]\u001b[A\n",
            " 36% 9/25 [00:04<00:08,  1.91it/s]\u001b[A\n",
            " 40% 10/25 [00:04<00:08,  1.87it/s]\u001b[A\n",
            " 44% 11/25 [00:05<00:07,  1.84it/s]\u001b[A\n",
            " 48% 12/25 [00:06<00:07,  1.81it/s]\u001b[A\n",
            " 52% 13/25 [00:06<00:06,  1.79it/s]\u001b[A\n",
            " 56% 14/25 [00:07<00:06,  1.68it/s]\u001b[A\n",
            " 60% 15/25 [00:07<00:06,  1.64it/s]\u001b[A\n",
            " 64% 16/25 [00:08<00:05,  1.79it/s]\u001b[A\n",
            " 68% 17/25 [00:08<00:04,  1.88it/s]\u001b[A\n",
            " 72% 18/25 [00:09<00:03,  1.98it/s]\u001b[A\n",
            " 76% 19/25 [00:09<00:02,  2.00it/s]\u001b[A\n",
            " 80% 20/25 [00:10<00:02,  1.99it/s]\u001b[A\n",
            " 84% 21/25 [00:10<00:02,  1.86it/s]\u001b[A\n",
            " 88% 22/25 [00:11<00:01,  1.93it/s]\u001b[A\n",
            " 92% 23/25 [00:11<00:01,  1.97it/s]\u001b[A\n",
            " 96% 24/25 [00:12<00:00,  1.97it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.138242244720459, 'eval_bleu': 12.5127, 'eval_gen_len': 12.77, 'eval_runtime': 13.4563, 'eval_samples_per_second': 29.726, 'eval_steps_per_second': 1.858, 'epoch': 1.13}\n",
            " 57% 4000/7064 [24:31<15:13,  3.35it/s]\n",
            "100% 25/25 [00:12<00:00,  1.98it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/regMADAR/checkpoint-4000\n",
            "Configuration saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-4000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-4000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/regMADAR/checkpoint-4000/spiece.model\n",
            " 71% 5000/7064 [30:42<11:14,  3.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:07,  3.15it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:09,  2.35it/s]\u001b[A\n",
            " 16% 4/25 [00:01<00:10,  1.98it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:11,  1.79it/s]\u001b[A\n",
            " 24% 6/25 [00:03<00:11,  1.58it/s]\u001b[A\n",
            " 28% 7/25 [00:04<00:12,  1.46it/s]\u001b[A\n",
            " 32% 8/25 [00:04<00:10,  1.61it/s]\u001b[A\n",
            " 36% 9/25 [00:05<00:09,  1.77it/s]\u001b[A\n",
            " 40% 10/25 [00:05<00:08,  1.87it/s]\u001b[A\n",
            " 44% 11/25 [00:05<00:07,  1.99it/s]\u001b[A\n",
            " 48% 12/25 [00:06<00:06,  2.04it/s]\u001b[A\n",
            " 52% 13/25 [00:06<00:05,  2.12it/s]\u001b[A\n",
            " 56% 14/25 [00:07<00:05,  2.13it/s]\u001b[A\n",
            " 60% 15/25 [00:07<00:04,  2.14it/s]\u001b[A\n",
            " 64% 16/25 [00:08<00:04,  2.16it/s]\u001b[A\n",
            " 68% 17/25 [00:08<00:03,  2.14it/s]\u001b[A\n",
            " 72% 18/25 [00:09<00:03,  2.20it/s]\u001b[A\n",
            " 76% 19/25 [00:09<00:02,  2.16it/s]\u001b[A\n",
            " 80% 20/25 [00:10<00:02,  2.10it/s]\u001b[A\n",
            " 84% 21/25 [00:10<00:02,  1.91it/s]\u001b[A\n",
            " 88% 22/25 [00:11<00:01,  1.97it/s]\u001b[A\n",
            " 92% 23/25 [00:11<00:01,  1.99it/s]\u001b[A\n",
            " 96% 24/25 [00:12<00:00,  1.99it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.1527037620544434, 'eval_bleu': 11.8951, 'eval_gen_len': 12.72, 'eval_runtime': 13.2799, 'eval_samples_per_second': 30.121, 'eval_steps_per_second': 1.883, 'epoch': 1.42}\n",
            " 71% 5000/7064 [30:55<11:14,  3.06it/s]\n",
            "100% 25/25 [00:12<00:00,  1.98it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/regMADAR/checkpoint-5000\n",
            "Configuration saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-5000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-5000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/regMADAR/checkpoint-5000/spiece.model\n",
            " 85% 6000/7064 [36:54<05:54,  3.00it/s]***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:08,  2.62it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:09,  2.38it/s]\u001b[A\n",
            " 16% 4/25 [00:01<00:09,  2.20it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.09it/s]\u001b[A\n",
            " 24% 6/25 [00:02<00:09,  1.94it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:09,  1.84it/s]\u001b[A\n",
            " 32% 8/25 [00:03<00:08,  1.90it/s]\u001b[A\n",
            " 36% 9/25 [00:04<00:08,  1.98it/s]\u001b[A\n",
            " 40% 10/25 [00:04<00:07,  2.03it/s]\u001b[A\n",
            " 44% 11/25 [00:05<00:06,  2.10it/s]\u001b[A\n",
            " 48% 12/25 [00:05<00:06,  2.11it/s]\u001b[A\n",
            " 52% 13/25 [00:06<00:05,  2.21it/s]\u001b[A\n",
            " 56% 14/25 [00:06<00:05,  2.16it/s]\u001b[A\n",
            " 60% 15/25 [00:07<00:04,  2.18it/s]\u001b[A\n",
            " 64% 16/25 [00:07<00:04,  2.20it/s]\u001b[A\n",
            " 68% 17/25 [00:08<00:03,  2.19it/s]\u001b[A\n",
            " 72% 18/25 [00:08<00:03,  2.24it/s]\u001b[A\n",
            " 76% 19/25 [00:08<00:02,  2.19it/s]\u001b[A\n",
            " 80% 20/25 [00:09<00:02,  2.13it/s]\u001b[A\n",
            " 84% 21/25 [00:10<00:02,  1.95it/s]\u001b[A\n",
            " 88% 22/25 [00:10<00:01,  2.00it/s]\u001b[A\n",
            " 92% 23/25 [00:11<00:01,  1.94it/s]\u001b[A\n",
            " 96% 24/25 [00:11<00:00,  1.83it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.159769296646118, 'eval_bleu': 12.0602, 'eval_gen_len': 12.7925, 'eval_runtime': 13.1573, 'eval_samples_per_second': 30.401, 'eval_steps_per_second': 1.9, 'epoch': 1.7}\n",
            " 85% 6000/7064 [37:07<05:54,  3.00it/s]\n",
            "100% 25/25 [00:12<00:00,  1.78it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/regMADAR/checkpoint-6000\n",
            "Configuration saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-6000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-6000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/regMADAR/checkpoint-6000/spiece.model\n",
            " 99% 7000/7064 [43:21<00:19,  3.31it/s]***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:06,  3.65it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:07,  2.81it/s]\u001b[A\n",
            " 16% 4/25 [00:01<00:08,  2.43it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.21it/s]\u001b[A\n",
            " 24% 6/25 [00:02<00:09,  2.00it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:09,  1.88it/s]\u001b[A\n",
            " 32% 8/25 [00:03<00:08,  1.92it/s]\u001b[A\n",
            " 36% 9/25 [00:04<00:07,  2.01it/s]\u001b[A\n",
            " 40% 10/25 [00:04<00:07,  2.06it/s]\u001b[A\n",
            " 44% 11/25 [00:05<00:06,  2.09it/s]\u001b[A\n",
            " 48% 12/25 [00:05<00:06,  1.98it/s]\u001b[A\n",
            " 52% 13/25 [00:06<00:06,  1.93it/s]\u001b[A\n",
            " 56% 14/25 [00:06<00:05,  1.87it/s]\u001b[A\n",
            " 60% 15/25 [00:07<00:05,  1.76it/s]\u001b[A\n",
            " 64% 16/25 [00:08<00:05,  1.68it/s]\u001b[A\n",
            " 68% 17/25 [00:08<00:04,  1.65it/s]\u001b[A\n",
            " 72% 18/25 [00:09<00:03,  1.81it/s]\u001b[A\n",
            " 76% 19/25 [00:09<00:03,  1.88it/s]\u001b[A\n",
            " 80% 20/25 [00:10<00:02,  1.89it/s]\u001b[A\n",
            " 84% 21/25 [00:10<00:02,  1.79it/s]\u001b[A\n",
            " 88% 22/25 [00:11<00:01,  1.87it/s]\u001b[A\n",
            " 92% 23/25 [00:11<00:01,  1.93it/s]\u001b[A\n",
            " 96% 24/25 [00:12<00:00,  1.92it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.170121669769287, 'eval_bleu': 11.5788, 'eval_gen_len': 12.665, 'eval_runtime': 13.3884, 'eval_samples_per_second': 29.877, 'eval_steps_per_second': 1.867, 'epoch': 1.98}\n",
            " 99% 7000/7064 [43:35<00:19,  3.31it/s]\n",
            "100% 25/25 [00:12<00:00,  1.95it/s]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/regMADAR/checkpoint-7000\n",
            "Configuration saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-7000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/regMADAR/checkpoint-7000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/regMADAR/checkpoint-7000/spiece.model\n",
            "{'loss': 1.4394, 'learning_rate': 0.0, 'epoch': 2.0}\n",
            "100% 7064/7064 [44:34<00:00,  3.24it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from s2/AraT5_baseV2_100/regMADAR/checkpoint-1000 (score: 13.7904).\n",
            "{'train_runtime': 2681.6286, 'train_samples_per_second': 42.142, 'train_steps_per_second': 2.634, 'train_loss': 1.8047389962530083, 'epoch': 2.0}\n",
            "100% 7064/7064 [44:41<00:00,  2.63it/s]\n",
            "Saving model checkpoint to s2/AraT5_baseV2_100/regMADAR\n",
            "Configuration saved in s2/AraT5_baseV2_100/regMADAR/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/regMADAR/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/regMADAR/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/regMADAR/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/regMADAR/spiece.model\n",
            "10/18/2023 16:30:46 - INFO - __main__ -   ***** train metrics *****\n",
            "10/18/2023 16:30:46 - INFO - __main__ -     epoch                    =        2.0\n",
            "10/18/2023 16:30:46 - INFO - __main__ -     train_loss               =     1.8047\n",
            "10/18/2023 16:30:46 - INFO - __main__ -     train_runtime            = 0:44:41.62\n",
            "10/18/2023 16:30:46 - INFO - __main__ -     train_samples            =      56505\n",
            "10/18/2023 16:30:46 - INFO - __main__ -     train_samples_per_second =     42.142\n",
            "10/18/2023 16:30:46 - INFO - __main__ -     train_steps_per_second   =      2.634\n",
            "10/18/2023 16:30:46 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "100% 25/25 [00:34<00:00,  1.38s/it]\n",
            "10/18/2023 16:31:22 - INFO - __main__ -   ***** val metrics *****\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     epoch                   =               2.0\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     eval_bleu               =           12.0938\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     eval_check_point        = AraT5v2-base-1024\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     eval_gen_len            =            16.045\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     eval_loss               =            3.1145\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     eval_runtime            =        0:00:35.37\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     eval_samples            =               400\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     eval_samples_per_second =            11.309\n",
            "10/18/2023 16:31:22 - INFO - __main__ -     eval_steps_per_second   =             0.707\n",
            "10/18/2023 16:31:22 - INFO - __main__ -   *** Test ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "100% 25/25 [00:34<00:00,  1.36s/it]\n",
            "10/18/2023 16:31:57 - INFO - __main__ -   ***** test metrics *****\n",
            "10/18/2023 16:31:57 - INFO - __main__ -     test_bleu               =           12.0938\n",
            "10/18/2023 16:31:57 - INFO - __main__ -     test_check_point        = AraT5v2-base-1024\n",
            "10/18/2023 16:31:57 - INFO - __main__ -     test_gen_len            =            16.045\n",
            "10/18/2023 16:31:57 - INFO - __main__ -     test_loss               =            3.1145\n",
            "10/18/2023 16:31:57 - INFO - __main__ -     test_runtime            =        0:00:35.39\n",
            "10/18/2023 16:31:57 - INFO - __main__ -     test_samples            =               400\n",
            "10/18/2023 16:31:57 - INFO - __main__ -     test_samples_per_second =            11.301\n",
            "10/18/2023 16:31:57 - INFO - __main__ -     test_steps_per_second   =             0.706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training an independent model for each regional dialect (I-R)\n",
        "<br> Split the NADI-dev dataset to validated each model"
      ],
      "metadata": {
        "id": "Md65HNUdSX4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on Gulf and validate on Emirates_NADI-dev\n",
        "!python run_trainier_seq2seq_huggingface.py \\\n",
        "        --learning_rate 5e-5 \\\n",
        "        --max_target_length 128 --max_source_length 128 \\\n",
        "        --per_device_train_batch_size 16 --per_device_eval_batch_size 16 \\\n",
        "        --model_name_or_path \"UBC-NLP/AraT5v2-base-1024\" \\\n",
        "        --output_dir \"s2/AraT5_baseV2_100/emiMADAR\" --overwrite_output_dir \\\n",
        "        --save_steps 1000 \\\n",
        "        --eval_steps 1000 \\\n",
        "        --num_train_epochs 2 \\\n",
        "        --train_file \"datasets/glfMADARmsa.csv\" \\\n",
        "        --validation_file \"datasets/emiNADI_dev.csv\" \\\n",
        "        --test_file \"datasets/emiNADI_dev.csv\" \\\n",
        "        --task \"machine_translation\" --text_column \"0\" --summary_column \"1\" \\\n",
        "        --load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy steps --logging_strategy epoch --predict_with_generate\\\n",
        "        --do_train --do_eval --do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klDOUUngSW1K",
        "outputId": "d562e829-2164-47a8-b89e-9b424c20a2ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-18 16:32:04.069212: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-18 16:32:05.769617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "last_checkpoint None\n",
            "10/18/2023 16:32:09 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/18/2023 16:32:09 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=s2/AraT5_baseV2_100/emiMADAR/runs/Oct18_16-32-09_b6e1481f22fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=epoch,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_bleu,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=s2/AraT5_baseV2_100/emiMADAR,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=s2/AraT5_baseV2_100/emiMADAR,\n",
            "save_on_each_node=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 20867.18it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 501.09it/s]\n",
            "Generating train split: 25616 examples [00:00, 241530.12 examples/s]\n",
            "Generating validation split: 100 examples [00:00, 33697.31 examples/s]\n",
            "Generating test split: 100 examples [00:00, 36567.60 examples/s]\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5v2-base-1024\",\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110208\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/spiece.model\n",
            "loading file tokenizer.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/tokenizer_config.json\n",
            "loading weights file pytorch_model.bin from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5v2-base-1024.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Map:   0% 0/25616 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 25616/25616 [00:02<00:00, 11915.50 examples/s]\n",
            "Map: 100% 100/100 [00:00<00:00, 6880.76 examples/s]\n",
            "Map: 100% 100/100 [00:00<00:00, 7917.81 examples/s]\n",
            "[INFO] evlaute using  bleu score task name: machine_translation\n",
            "[INFO] early_stopping_num= 20\n",
            "***** checkpoint= None\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 25616\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3202\n",
            "  Number of trainable parameters = 367508736\n",
            "  0% 0/3202 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 31% 1000/3202 [05:28<11:37,  3.16it/s]***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.40it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.47it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.03it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.81it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.93it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.0228769779205322, 'eval_bleu': 16.8732, 'eval_gen_len': 11.69, 'eval_runtime': 3.7862, 'eval_samples_per_second': 26.411, 'eval_steps_per_second': 1.849, 'epoch': 0.62}\n",
            " 31% 1000/3202 [05:32<11:37,  3.16it/s]\n",
            "100% 7/7 [00:03<00:00,  2.25it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/emiMADAR/checkpoint-1000\n",
            "Configuration saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-1000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/emiMADAR/checkpoint-1000/spiece.model\n",
            "{'loss': 2.1374, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
            " 62% 2000/3202 [11:39<06:18,  3.17it/s]***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.62it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:01,  2.89it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.61it/s]\u001b[A\n",
            " 71% 5/7 [00:01<00:00,  2.45it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.40it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.1294798851013184, 'eval_bleu': 15.472, 'eval_gen_len': 11.64, 'eval_runtime': 3.3388, 'eval_samples_per_second': 29.951, 'eval_steps_per_second': 2.097, 'epoch': 1.25}\n",
            " 62% 2000/3202 [11:42<06:18,  3.17it/s]\n",
            "100% 7/7 [00:02<00:00,  2.63it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/emiMADAR/checkpoint-2000\n",
            "Configuration saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-2000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-2000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/emiMADAR/checkpoint-2000/spiece.model\n",
            " 94% 3000/3202 [17:54<01:07,  3.00it/s]***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  4.12it/s]\u001b[A\n",
            " 43% 3/7 [00:00<00:01,  2.96it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.62it/s]\u001b[A\n",
            " 71% 5/7 [00:01<00:00,  2.44it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  2.39it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.0494678020477295, 'eval_bleu': 15.0127, 'eval_gen_len': 11.69, 'eval_runtime': 3.0432, 'eval_samples_per_second': 32.86, 'eval_steps_per_second': 2.3, 'epoch': 1.87}\n",
            " 94% 3000/3202 [17:57<01:07,  3.00it/s]\n",
            "100% 7/7 [00:02<00:00,  2.73it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/emiMADAR/checkpoint-3000\n",
            "Configuration saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-3000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/emiMADAR/checkpoint-3000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/emiMADAR/checkpoint-3000/spiece.model\n",
            "{'loss': 1.3349, 'learning_rate': 0.0, 'epoch': 2.0}\n",
            "100% 3202/3202 [19:42<00:00,  2.93it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from s2/AraT5_baseV2_100/emiMADAR/checkpoint-1000 (score: 16.8732).\n",
            "{'train_runtime': 1189.531, 'train_samples_per_second': 43.069, 'train_steps_per_second': 2.692, 'train_loss': 1.736155960874063, 'epoch': 2.0}\n",
            "100% 3202/3202 [19:49<00:00,  2.69it/s]\n",
            "Saving model checkpoint to s2/AraT5_baseV2_100/emiMADAR\n",
            "Configuration saved in s2/AraT5_baseV2_100/emiMADAR/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/emiMADAR/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/emiMADAR/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/emiMADAR/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/emiMADAR/spiece.model\n",
            "10/18/2023 16:52:29 - INFO - __main__ -   ***** train metrics *****\n",
            "10/18/2023 16:52:29 - INFO - __main__ -     epoch                    =        2.0\n",
            "10/18/2023 16:52:29 - INFO - __main__ -     train_loss               =     1.7362\n",
            "10/18/2023 16:52:29 - INFO - __main__ -     train_runtime            = 0:19:49.53\n",
            "10/18/2023 16:52:29 - INFO - __main__ -     train_samples            =      25616\n",
            "10/18/2023 16:52:29 - INFO - __main__ -     train_samples_per_second =     43.069\n",
            "10/18/2023 16:52:29 - INFO - __main__ -     train_steps_per_second   =      2.692\n",
            "10/18/2023 16:52:29 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "100% 7/7 [00:04<00:00,  1.42it/s]\n",
            "10/18/2023 16:52:36 - INFO - __main__ -   ***** val metrics *****\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     epoch                   =               2.0\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     eval_bleu               =           11.7051\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     eval_check_point        = AraT5v2-base-1024\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     eval_gen_len            =             16.59\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     eval_loss               =            3.0229\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     eval_runtime            =        0:00:07.25\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     eval_samples            =               100\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     eval_samples_per_second =            13.779\n",
            "10/18/2023 16:52:36 - INFO - __main__ -     eval_steps_per_second   =             0.965\n",
            "10/18/2023 16:52:36 - INFO - __main__ -   *** Test ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "100% 7/7 [00:05<00:00,  1.28it/s]\n",
            "10/18/2023 16:52:44 - INFO - __main__ -   ***** test metrics *****\n",
            "10/18/2023 16:52:44 - INFO - __main__ -     test_bleu               =           11.7051\n",
            "10/18/2023 16:52:44 - INFO - __main__ -     test_check_point        = AraT5v2-base-1024\n",
            "10/18/2023 16:52:44 - INFO - __main__ -     test_gen_len            =             16.59\n",
            "10/18/2023 16:52:44 - INFO - __main__ -     test_loss               =            3.0229\n",
            "10/18/2023 16:52:44 - INFO - __main__ -     test_runtime            =        0:00:08.07\n",
            "10/18/2023 16:52:44 - INFO - __main__ -     test_samples            =               100\n",
            "10/18/2023 16:52:44 - INFO - __main__ -     test_samples_per_second =            12.384\n",
            "10/18/2023 16:52:44 - INFO - __main__ -     test_steps_per_second   =             0.867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on Egyption and validate on Egyption_NADI-dev\n",
        "!python run_trainier_seq2seq_huggingface.py \\\n",
        "        --learning_rate 5e-5 \\\n",
        "        --max_target_length 128 --max_source_length 128 \\\n",
        "        --per_device_train_batch_size 16 --per_device_eval_batch_size 16 \\\n",
        "        --model_name_or_path \"UBC-NLP/AraT5v2-base-1024\" \\\n",
        "        --output_dir \"s2/AraT5_baseV2_100/egyMADAR\" --overwrite_output_dir \\\n",
        "        --save_steps 1000 \\\n",
        "        --eval_steps 1000 \\\n",
        "        --num_train_epochs 2 \\\n",
        "        --train_file \"datasets/egyMADARmsa.csv\" \\\n",
        "        --validation_file \"datasets/egyNADI_dev.csv\" \\\n",
        "        --test_file \"datasets/egyNADI_dev.csv\" \\\n",
        "        --task \"machine_translation\" --text_column \"0\" --summary_column \"1\" \\\n",
        "        --load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy steps --logging_strategy epoch --predict_with_generate\\\n",
        "        --do_train --do_eval --do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3UOS3OQShTD",
        "outputId": "f0ec04d4-2609-4fe6-8a77-8ecb5612e987"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-18 16:52:50.988345: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-18 16:52:53.063251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "last_checkpoint None\n",
            "10/18/2023 16:52:57 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/18/2023 16:52:57 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=s2/AraT5_baseV2_100/egyMADAR/runs/Oct18_16-52-57_b6e1481f22fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=epoch,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_bleu,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=s2/AraT5_baseV2_100/egyMADAR,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=s2/AraT5_baseV2_100/egyMADAR,\n",
            "save_on_each_node=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 23786.22it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2435.72it/s]\n",
            "Generating train split: 24148 examples [00:00, 242484.44 examples/s]\n",
            "Generating validation split: 100 examples [00:00, 32378.45 examples/s]\n",
            "Generating test split: 100 examples [00:00, 36853.56 examples/s]\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5v2-base-1024\",\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110208\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/spiece.model\n",
            "loading file tokenizer.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/tokenizer_config.json\n",
            "loading weights file pytorch_model.bin from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5v2-base-1024.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Map:   0% 0/24148 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 24148/24148 [00:02<00:00, 11705.70 examples/s]\n",
            "Map: 100% 100/100 [00:00<00:00, 6378.01 examples/s]\n",
            "Map: 100% 100/100 [00:00<00:00, 5174.19 examples/s]\n",
            "[INFO] evlaute using  bleu score task name: machine_translation\n",
            "[INFO] early_stopping_num= 20\n",
            "***** checkpoint= None\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 24148\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3020\n",
            "  Number of trainable parameters = 367508736\n",
            "  0% 0/3020 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 33% 1000/3020 [05:30<10:59,  3.06it/s]***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.69it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.79it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.42it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.21it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.89it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.0291173458099365, 'eval_bleu': 13.1094, 'eval_gen_len': 11.77, 'eval_runtime': 3.7971, 'eval_samples_per_second': 26.336, 'eval_steps_per_second': 1.844, 'epoch': 0.66}\n",
            " 33% 1000/3020 [05:33<10:59,  3.06it/s]\n",
            "100% 7/7 [00:03<00:00,  1.96it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/egyMADAR/checkpoint-1000\n",
            "Configuration saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-1000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/egyMADAR/checkpoint-1000/spiece.model\n",
            "{'loss': 2.1508, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
            " 66% 2000/3020 [11:37<05:21,  3.17it/s]***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.59it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.75it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.38it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.00it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.75it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.9306845664978027, 'eval_bleu': 12.7104, 'eval_gen_len': 12.36, 'eval_runtime': 4.0869, 'eval_samples_per_second': 24.468, 'eval_steps_per_second': 1.713, 'epoch': 1.32}\n",
            " 66% 2000/3020 [11:42<05:21,  3.17it/s]\n",
            "100% 7/7 [00:03<00:00,  1.75it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/egyMADAR/checkpoint-2000\n",
            "Configuration saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-2000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-2000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/egyMADAR/checkpoint-2000/spiece.model\n",
            " 99% 3000/3020 [17:51<00:06,  2.98it/s]***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  3.60it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.80it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  2.40it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:00,  2.20it/s]\u001b[A\n",
            " 86% 6/7 [00:02<00:00,  1.99it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.924842596054077, 'eval_bleu': 13.1877, 'eval_gen_len': 12.39, 'eval_runtime': 3.8363, 'eval_samples_per_second': 26.067, 'eval_steps_per_second': 1.825, 'epoch': 1.99}\n",
            " 99% 3000/3020 [17:54<00:06,  2.98it/s]\n",
            "100% 7/7 [00:03<00:00,  2.10it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/egyMADAR/checkpoint-3000\n",
            "Configuration saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-3000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/egyMADAR/checkpoint-3000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/egyMADAR/checkpoint-3000/spiece.model\n",
            "{'loss': 1.3089, 'learning_rate': 0.0, 'epoch': 2.0}\n",
            "100% 3020/3020 [18:34<00:00,  3.20it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from s2/AraT5_baseV2_100/egyMADAR/checkpoint-3000 (score: 13.1877).\n",
            "{'train_runtime': 1120.1933, 'train_samples_per_second': 43.114, 'train_steps_per_second': 2.696, 'train_loss': 1.729865350786424, 'epoch': 2.0}\n",
            "100% 3020/3020 [18:40<00:00,  2.70it/s]\n",
            "Saving model checkpoint to s2/AraT5_baseV2_100/egyMADAR\n",
            "Configuration saved in s2/AraT5_baseV2_100/egyMADAR/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/egyMADAR/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/egyMADAR/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/egyMADAR/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/egyMADAR/spiece.model\n",
            "10/18/2023 17:12:04 - INFO - __main__ -   ***** train metrics *****\n",
            "10/18/2023 17:12:04 - INFO - __main__ -     epoch                    =        2.0\n",
            "10/18/2023 17:12:04 - INFO - __main__ -     train_loss               =     1.7299\n",
            "10/18/2023 17:12:04 - INFO - __main__ -     train_runtime            = 0:18:40.19\n",
            "10/18/2023 17:12:04 - INFO - __main__ -     train_samples            =      24148\n",
            "10/18/2023 17:12:04 - INFO - __main__ -     train_samples_per_second =     43.114\n",
            "10/18/2023 17:12:04 - INFO - __main__ -     train_steps_per_second   =      2.696\n",
            "10/18/2023 17:12:04 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "100% 7/7 [00:04<00:00,  1.43it/s]\n",
            "10/18/2023 17:12:10 - INFO - __main__ -   ***** val metrics *****\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     epoch                   =               2.0\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     eval_bleu               =           16.5286\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     eval_check_point        = AraT5v2-base-1024\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     eval_gen_len            =             14.12\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     eval_loss               =            2.9248\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     eval_runtime            =        0:00:05.84\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     eval_samples            =               100\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     eval_samples_per_second =            17.094\n",
            "10/18/2023 17:12:10 - INFO - __main__ -     eval_steps_per_second   =             1.197\n",
            "10/18/2023 17:12:10 - INFO - __main__ -   *** Test ***\n",
            "***** Running Prediction *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "100% 7/7 [00:05<00:00,  1.19it/s]\n",
            "10/18/2023 17:12:17 - INFO - __main__ -   ***** test metrics *****\n",
            "10/18/2023 17:12:17 - INFO - __main__ -     test_bleu               =           16.5286\n",
            "10/18/2023 17:12:17 - INFO - __main__ -     test_check_point        = AraT5v2-base-1024\n",
            "10/18/2023 17:12:17 - INFO - __main__ -     test_gen_len            =             14.12\n",
            "10/18/2023 17:12:17 - INFO - __main__ -     test_loss               =            2.9248\n",
            "10/18/2023 17:12:17 - INFO - __main__ -     test_runtime            =        0:00:06.79\n",
            "10/18/2023 17:12:17 - INFO - __main__ -     test_samples            =               100\n",
            "10/18/2023 17:12:17 - INFO - __main__ -     test_samples_per_second =            14.712\n",
            "10/18/2023 17:12:17 - INFO - __main__ -     test_steps_per_second   =              1.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on Levantine and validate on palestinian_NADI-dev\n",
        "!python run_trainier_seq2seq_huggingface.py \\\n",
        "        --learning_rate 5e-5 \\\n",
        "        --max_target_length 128 --max_source_length 128 \\\n",
        "        --per_device_train_batch_size 16 --per_device_eval_batch_size 16 \\\n",
        "        --model_name_or_path \"UBC-NLP/AraT5v2-base-1024\" \\\n",
        "        --output_dir \"s2/AraT5_baseV2_100/palMADAR\" --overwrite_output_dir \\\n",
        "        --save_steps 1000 \\\n",
        "        --eval_steps 1000 \\\n",
        "        --num_train_epochs 2 \\\n",
        "        --train_file \"datasets/levMADARmsa.csv\" \\\n",
        "        --validation_file \"datasets/palNADI_dev.csv\" \\\n",
        "        --test_file \"datasets/palNADI_dev.csv\" \\\n",
        "        --task \"machine_translation\" --text_column \"0\" --summary_column \"1\" \\\n",
        "        --load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy steps --logging_strategy epoch --predict_with_generate\\\n",
        "        --do_train --do_eval --do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP9nUgB4ShjY",
        "outputId": "9c7b20b6-bb55-4ce1-f40e-77cd27c64287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-18 17:12:23.804947: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-18 17:12:25.447049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "last_checkpoint None\n",
            "10/18/2023 17:12:30 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "10/18/2023 17:12:30 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=s2/AraT5_baseV2_100/palMADAR/runs/Oct18_17-12-30_b6e1481f22fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=epoch,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_bleu,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=s2/AraT5_baseV2_100/palMADAR,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=s2/AraT5_baseV2_100/palMADAR,\n",
            "save_on_each_node=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 24059.11it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 2276.21it/s]\n",
            "Generating train split: 28402 examples [00:00, 246588.43 examples/s]\n",
            "Generating validation split: 100 examples [00:00, 31410.95 examples/s]\n",
            "Generating test split: 100 examples [00:00, 35439.83 examples/s]\n",
            "loading configuration file config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"UBC-NLP/AraT5v2-base-1024\",\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 110208\n",
            "}\n",
            "\n",
            "loading file spiece.model from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/spiece.model\n",
            "loading file tokenizer.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/tokenizer_config.json\n",
            "loading weights file pytorch_model.bin from cache at /tmp/AraT5_cache_dir/models--UBC-NLP--AraT5v2-base-1024/snapshots/d23d48d6da0cc847697f5764020c3a76aa7222d7/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at UBC-NLP/AraT5v2-base-1024.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Map:   0% 0/28402 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3546: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100% 28402/28402 [00:02<00:00, 12149.33 examples/s]\n",
            "Map: 100% 100/100 [00:00<00:00, 6283.04 examples/s]\n",
            "Map: 100% 100/100 [00:00<00:00, 6416.25 examples/s]\n",
            "[INFO] evlaute using  bleu score task name: machine_translation\n",
            "[INFO] early_stopping_num= 20\n",
            "***** checkpoint= None\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 28402\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3552\n",
            "  Number of trainable parameters = 367508736\n",
            "  0% 0/3552 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 28% 1000/3552 [05:29<14:56,  2.85it/s]***** Running Evaluation *****\n",
            "  Num examples = 100\n",
            "  Batch size = 16\n",
            "\n",
            "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 2/7 [00:00<00:01,  2.78it/s]\u001b[A\n",
            " 43% 3/7 [00:01<00:01,  2.14it/s]\u001b[A\n",
            " 57% 4/7 [00:01<00:01,  1.91it/s]\u001b[A\n",
            " 71% 5/7 [00:02<00:01,  1.70it/s]\u001b[A\n",
            " 86% 6/7 [00:03<00:00,  1.62it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 3.1836442947387695, 'eval_bleu': 16.0143, 'eval_gen_len': 13.95, 'eval_runtime': 4.3807, 'eval_samples_per_second': 22.828, 'eval_steps_per_second': 1.598, 'epoch': 0.56}\n",
            " 28% 1000/3552 [05:34<14:56,  2.85it/s]\n",
            "100% 7/7 [00:03<00:00,  1.81it/s]\u001b[A\n",
            "                                 \u001b[ASaving model checkpoint to s2/AraT5_baseV2_100/palMADAR/checkpoint-1000\n",
            "Configuration saved in s2/AraT5_baseV2_100/palMADAR/checkpoint-1000/config.json\n",
            "Model weights saved in s2/AraT5_baseV2_100/palMADAR/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in s2/AraT5_baseV2_100/palMADAR/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in s2/AraT5_baseV2_100/palMADAR/checkpoint-1000/special_tokens_map.json\n",
            "Copy vocab file to s2/AraT5_baseV2_100/palMADAR/checkpoint-1000/spiece.model\n",
            " 33% 1156/3552 [06:46<12:42,  3.14it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on Levantine and validate on Jordanian_NADI-dev\n",
        "!python run_trainier_seq2seq_huggingface.py \\\n",
        "        --learning_rate 5e-5 \\\n",
        "        --max_target_length 128 --max_source_length 128 \\\n",
        "        --per_device_train_batch_size 16 --per_device_eval_batch_size 16 \\\n",
        "        --model_name_or_path \"UBC-NLP/AraT5v2-base-1024\" \\\n",
        "        --output_dir \"s2/AraT5_baseV2_100/jorMADAR\" --overwrite_output_dir \\\n",
        "        --save_steps 1000 \\\n",
        "        --eval_steps 1000 \\\n",
        "        --num_train_epochs 2 \\\n",
        "        --train_file \"datasets/levMADARmsa.csv\" \\\n",
        "        --validation_file \"datasets/jorNADI_dev.csv\" \\\n",
        "        --test_file \"datasets/jorNADI_dev.csv\" \\\n",
        "        --task \"machine_translation\" --text_column \"0\" --summary_column \"1\" \\\n",
        "        --load_best_model_at_end --metric_for_best_model \"eval_bleu\" --greater_is_better True --evaluation_strategy steps --logging_strategy epoch --predict_with_generate\\\n",
        "        --do_train --do_eval --do_predict"
      ],
      "metadata": {
        "id": "B_aK8iW1UJNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}